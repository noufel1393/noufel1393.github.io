<!DOCTYPE html>
<html lang="en-us">

  <head>
  <meta charset="utf-8">
  <meta name="robots" content="all,follow">
  <meta name="googlebot" content="index,follow,snippet,archive">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Spark Repartition &amp; Coalesce - Explained</title>
  <meta name="author" content="" />

  
  <meta name="keywords" content="devows, hugo, go">	
  

  
  <meta name="description" content="Site template made by devcows using hugo">	
  

  <meta name="generator" content="Hugo 0.52" />

  <link href='//fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,500,700,800' rel='stylesheet' type='text/css'>

  
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">

  
  <link href="/css/animate.css" rel="stylesheet">

  
  
    <link href="/css/style.default.css" rel="stylesheet" id="theme-stylesheet">
  


  
  <link href="/css/custom.css" rel="stylesheet">

  
  
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  

  
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" href="/img/apple-touch-icon.png" />
  

  <link href="/css/owl.carousel.css" rel="stylesheet">
  <link href="/css/owl.theme.css" rel="stylesheet">

  <link rel="alternate" href="https://noufel1393.github.io/index.xml" type="application/rss+xml" title="Universal">

  
  <meta property="og:title" content="Spark Repartition &amp; Coalesce - Explained" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="/blog/2018/11/20/spark_repartition_coalesce//" />
  <meta property="og:image" content="img/logo.png" />

</head>


  <body>

    <div id="all">

        <header>

          <div class="navbar-affixed-top" data-spy="affix" data-offset-top="200">

    <div class="navbar navbar-default yamm" role="navigation" id="navbar">

        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand home" href="https://noufel1393.github.io">
                    <img src="https://noufel1393.github.ioimg/logo.png" alt="Spark Repartition &amp; Coalesce - Explained logo" class="hidden-xs hidden-sm">
                    <img src="https://noufel1393.github.ioimg/logo-small.png" alt="Spark Repartition &amp; Coalesce - Explained logo" class="visible-xs visible-sm">
                    <span class="sr-only">Spark Repartition &amp; Coalesce - Explained - go to homepage</span>
                </a>
                <div class="navbar-buttons">
                    <button type="button" class="navbar-toggle btn-template-main" data-toggle="collapse" data-target="#navigation">
                      <span class="sr-only">Toggle Navigation</span>
                        <i class="fa fa-align-justify"></i>
                    </button>
                </div>
            </div>
            

            <div class="navbar-collapse collapse" id="navigation">
                <ul class="nav navbar-nav navbar-right">
                  
                  <li class="dropdown">
                    
                    <a href="/">Home</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/blog/">Blog</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/faq/">FAQ</a>
                    
                  </li>
                  
                  <li class="dropdown">
                    
                    <a href="/contact/">Contact</a>
                    
                  </li>
                  
                </ul>
            </div>
            

            <div class="collapse clearfix" id="search">

                <form class="navbar-form" role="search">
                    <div class="input-group">
                        <input type="text" class="form-control" placeholder="Search">
                        <span class="input-group-btn">

                    <button type="submit" class="btn btn-template-main"><i class="fa fa-search"></i></button>

                </span>
                    </div>
                </form>

            </div>
            

        </div>
    </div>
    

</div>




        </header>

        <div id="heading-breadcrumbs">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h1>Spark Repartition &amp; Coalesce - Explained</h1>
            </div>
        </div>
    </div>
</div>


        <div id="content">
            <div class="container">

                <div class="row">

                    

                    <div class="col-md-9" id="blog-post">

                        <p class="text-muted text-uppercase mb-small text-right">November 20, 2018</p>

                        <div id="post-content">
                          

<p><em> All data processed by spark is stored in partitions. Today we discuss what are partitions, how partitioning works in Spark (Pyspark), why it matters and how the user can manually control the partitions using repartition and coalesce for effective distributed computing.</em></p>

<h2 id="introduction">Introduction</h2>

<p>Spark is a framework which provides parallel and distributed computing on big data. To perform it&rsquo;s parallel processing, spark splits the data into smaller chunks(i.e. partitions) and distributes the same to each node in the cluster to provide a parallel execution of the data. This partitioning of data is performed by spark&rsquo;s internals and the same can also be controlled by the user.</p>

<h2 id="table-of-contents">Table of Contents</h2>

<ul>
<li><a href="https://noufel1393.github.io/spark_repartition_coalesce.html#1-what-is-partitioning-in-spark">1. What is Partitioning in Spark</a>

<ul>
<li>How to see number of Partitions in a Dataframe</li>
<li>How to control the number of partitions when reading / writing files in Spark</li>
</ul></li>
<li><a href="https://noufel1393.github.io/spark_repartition_coalesce.html#2-why-is-partitioning-required">2. Why is Partitioning required</a></li>
<li><a href="https://noufel1393.github.io/spark_repartition_coalesce.html#3-repartition">3. Repartition</a>

<ul>
<li>What is Repartition?</li>
<li>Methods to Repartition a Dataframe</li>
</ul></li>
<li><a href="https://noufel1393.github.io/spark_repartition_coalesce.html#4-coalesce">4. Coalesce</a>

<ul>
<li>Can you increase the number of partitions using Coalesce?</li>
</ul></li>
<li><a href="https://noufel1393.github.io/spark_repartition_coalesce.html#5-repartition-vs-coalesce---when-to-use-what-">5. Repartition vs Coalesce - When to use what?</a></li>
<li><a href="https://noufel1393.github.io/spark_repartition_coalesce.html#6-conclusion">6. Conclusion</a></li>
</ul>

<h2 id="1-what-is-partitioning-in-spark">1. What is Partitioning in Spark ?</h2>

<p>A Partition in simple terms is a split in the input data, so partitions in spark are basically smaller logical chunks or divisions of the input data. Spark distributes this partitioned data among the different nodes to perform distributed processing on the data.</p>

<p>Let&rsquo;s dive into a practical example and create a simple RDD using the <code>sc.parallelize</code> method to determine the number of partitions spark creates by default.</p>

<pre><code class="language-python">rdd = sc.parallelize(range(1,11))
rdd.getNumPartitions()
</code></pre>

<pre><code>8
</code></pre>

<p><br>
As shown above, spark creates 8 partitions by default here. Now, you must be wondering where this default number is come from.</p>

<p>Spark has a default parallelism parameter which is determined by, <code>sc.defaultParallelism</code>. When we run this method, it returns 8 as shown below,</p>

<pre><code class="language-python">sc.defaultParallelism
</code></pre>

<pre><code>8
</code></pre>

<p><br>
This is how the data is present inside each partition. Spark uses an internal Hash Partitioning Scheme to split the data into these smaller chunks.</p>

<p>We can use the <code>rdd.glom()</code> method to display the partitions in a list.</p>

<p><code>glom</code> - returns an RDD having the elements within each partition in a separate list</p>

<pre><code class="language-python">rdd.glom().collect()
</code></pre>

<pre><code>[[1], [2], [3], [4, 5], [6], [7], [8], [9, 10]]
</code></pre>

<p>Let&rsquo;s visualize the above collect operation in the Spark WebUI(if you are using spark locally, navigate to <a href="https://localhost:4040">https://localhost:4040</a> to see the spark webui or if spark is being accessed via a cluster navigate to your cluster specific localhost webUI)</p>

<p>Note: Spark shell notifies a port number before creating a Sparksession.</p>

<p>Since 8 partitions are present, 8 executors would be launched for this action, as shown below,</p>

<p><img src="/image1.png" alt="image.png" /></p>

<h3 id="how-to-see-number-of-partitions-in-a-dataframe">How to see number of Partitions in a Dataframe ?</h3>

<p>The above method used for an RDD can also be applied to a dataframe. Let&rsquo;s convert the RDD to a Dataframe and apply the same method. We can see that the same number of partitions are present.</p>

<pre><code class="language-python">df = rdd.map(lambda x: (x, )).toDF() # converting rdd to Dataframe
df.rdd.glom().collect()
</code></pre>

<pre><code>[[Row(_1=1)],
 [Row(_1=2)],
 [Row(_1=3)],
 [Row(_1=4), Row(_1=5)],
 [Row(_1=6)],
 [Row(_1=7)],
 [Row(_1=8)],
 [Row(_1=9), Row(_1=10)]]
</code></pre>

<h3 id="how-to-control-the-number-of-partitions-when-reading-writing-files-in-spark">How to control the number of partitions when reading / writing files in Spark?</h3>

<p>Now, you must&rsquo;ve had the question in your mind as to how spark partitions the data when reading textfiles. Let&rsquo;s read a simple textfile and see the number of partitions here.</p>

<p>Let&rsquo;s read the CSV file which was the input dataset in my first post, [pyspark dataframe basic operations]</p>

<pre><code class="language-python">df2 = spark.read.format('csv').options(delimiter=',', header=True).load('/Users/ecom-ahmed.noufel/Downloads/population.csv')
df2.rdd.getNumPartitions()
</code></pre>

<pre><code>1
</code></pre>

<p><br>
We can see that only 1 partition is created here. Alright let&rsquo;s break this down,</p>

<p><em>Spark by default creates 1 partition for every 128 MB of the file.</em><br>
So if you are reading a file of size 1GB, it creates 10 partitions.</p>

<p><strong>So how to control the number of bytes a single partition can hold ?</strong></p>

<p>The file being read here is of 3.6 MB, hence only 1 partition is created in this case.</p>

<p>The no. of partitions is determined by <code>spark.sql.files.maxPartitionBytes</code> parameter, which is set to 128 MB, by default.</p>

<pre><code class="language-python">spark.conf.get(&quot;spark.sql.files.maxPartitionBytes&quot;) # -&gt; 128 MB by default
</code></pre>

<pre><code>'134217728'
</code></pre>

<p><strong><em>Note:</em></strong> <em>The files being read must be splittable by default for spark to create partitions when reading the file. So, in case of compressed files like snappy, gz or lzo etc, a single partition is created irrespective of the size of the file.</em></p>

<p>Let&rsquo;s manually set the <code>spark.sql.files.maxPartitionBytes</code> to 1MB, so that we get 4 partitions upon reading the file.</p>

<pre><code class="language-python">spark.conf.set(&quot;spark.sql.files.maxPartitionBytes&quot;, 1000000)
spark.conf.get(&quot;spark.sql.files.maxPartitionBytes&quot;)
</code></pre>

<pre><code>'1000000'
</code></pre>

<pre><code class="language-python">df3 = spark.read.format('csv').options(delimiter=',', header=True).load('/Users/ecom-ahmed.noufel/Downloads/population.csv')
df3.rdd.getNumPartitions()
</code></pre>

<pre><code>4
</code></pre>

<p>As discussed, 4 partitions were created.</p>

<p><strong>Caution:</strong> The above manual setting of the <code>spark.sql.files.maxPartitionBytes</code> to 1MB was just for the purpose of this demo and is not recommended in practical scenarios. It is recommended to use the default setting or set a value based on your input size and cluster hardware size.</p>

<h2 id="2-why-is-partitioning-required">2. Why is Partitioning required ?</h2>

<p>Partitioning is the sole basis by which spark distributes data among different nodes to thereby producing a distributed and parallel execution of the data with reduced latency.</p>

<p>If this concept of partitioning is not present in spark, then there would be no parallel processing existing in spark.</p>

<p>Let&rsquo;s analyze this with a simple example where we create an RDD with a single partition and perform an action on the same.</p>

<pre><code class="language-python">rdd2 = sc.parallelize(range(20), 1) # the second optional parameter specifies the number of partitions, which is 1 here
rdd2.collect()
</code></pre>

<pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
</code></pre>

<p>When the above action is seen on the Spark WebUI, only a single executor would be issued to process this data.</p>

<p>So, imagine a case where you are processing a huge volume of data without the concept of partitioning, then the entire data would be processed by a single executor taking a lot of time and memory.</p>

<p><img src="/image5.png" alt="image.png" /></p>

<h2 id="3-repartition">3. Repartition</h2>

<p>Now what if you wish to control these partitions by yourself. This is where the methods of <code>repartition</code> and <code>coalesce</code> come to effect.</p>

<h3 id="what-is-repartition">What is Repartition?</h3>

<p>Repartition is a method in spark which is used to perform a full shuffle on the data present and creates partitions based on the user&rsquo;s input.</p>

<p><em>The resulting data is hash partitioned and the data is equally distributed among the partitions.</em></p>

<h3 id="methods-to-repartition-a-dataframe">Methods to Repartition a Dataframe</h3>

<p>There are 2 ways to repartition a dataframe,
* Specifying a set of columns or column along with the partition size (or)
* Specifying an int value to create the number of partitions</p>

<p><strong>Method 1 : Repartition using Column Name</strong> <br>
Now let&rsquo;s repartition our dataset using the first method using the column present in the dataframe and check the number of partitions being created after repartition.</p>

<pre><code class="language-python"># df.printSchema()   #&gt; root
                     # |-- _1: long (nullable = true)

df = df.repartition('_1')
df.rdd.getNumPartitions()
</code></pre>

<pre><code>200
</code></pre>

<p><br>
After our repartition, 200 partitions are created by default. This is because of the value present in <code>spark.sql.shuffle.partitions</code>. Spark uses the value present here to create the number of partitions after the shuffle operation. By default, 200 partitions are created if the number is not specified in the repartition clause.</p>

<pre><code class="language-python">spark.conf.get(&quot;spark.sql.shuffle.partitions&quot;)
</code></pre>

<pre><code>'200'
</code></pre>

<p><br>
<strong>Method 2 : Repartition using integer value</strong> <br>
Now let&rsquo;s use method 2 to specify an integer value to create the number of partitions as per our requirement. Let&rsquo;s repartition the dataset to 3 partitions.</p>

<pre><code class="language-python">df = df.repartition(3)
df.rdd.glom().collect()
</code></pre>

<pre><code>[[Row(_1=5), Row(_1=6), Row(_1=7), Row(_1=9)],
 [Row(_1=1), Row(_1=2), Row(_1=4)],
 [Row(_1=3), Row(_1=8), Row(_1=10)]]
</code></pre>

<p>Now, if you save the above dataframe as CSV, 3 files would be created with each one having contents as below,</p>

<p>Partition1 : 5,6,7,9<br>
Partition2 : 1,2,4<br>
Partition3 : 3,8,10</p>

<p>Let&rsquo;s visualise the same on the Spark WebUI</p>

<p>The above execution when seen on the spark console would show 3 tasks being issued on the collect operation(stage 2).</p>

<p><img src="/image2.png" alt="image.png" /></p>

<p><em>Caution: Repartition performs a full shuffle on the input dataset, hence it would be a costly operation if done on large datasets.</em></p>

<p><em>Note: Use Repartition only when you want to increase the number of partitions (or) if you want to perform a full shuffle on the data.</em></p>

<h2 id="4-coalesce">4. Coalesce</h2>

<p>Coalesce is another method to partition the data in a dataframe. This is mainly used to <strong>reduce the number of partitions</strong> in a dataframe.</p>

<p>Unlike repartition, <strong>coalesce doesn&rsquo;t perform a shuffle</strong> to create the partitions.<br>
Suppose there are 100 partitions with 10 records in each partition, and if the partition size is reduced to 50, it would retain 50 partitions and append the other values to these existing partitions thereby having 20 records in each partition.</p>

<p>Let&rsquo;s analyze this using our dataset, let&rsquo;s reduce the number of partitions to 2 now,</p>

<pre><code class="language-python">df3 = df.coalesce(2)
df3.rdd.glom().collect()
</code></pre>

<pre><code>[[Row(_1=5), Row(_1=6), Row(_1=7), Row(_1=9)],
 [Row(_1=1), Row(_1=2), Row(_1=4), Row(_1=3), Row(_1=8), Row(_1=10)]]
</code></pre>

<p><br>
As shown above, the data from the 3rd partition is removed and appended with the 2nd partition, proving that there is no shuffle process going on here. Hence, this would be much less memory intensive.</p>

<p>Now, if you save the above dataframe as CSV, 3 files would be created with each one having contents as below,</p>

<p>Partition1 : 5,6,7,9 <br>
Partition2 : 1,2,4,3,8,10</p>

<p>So, when your motive is to reduce the number of partitions from say 1000 to 500, then coalesce is a better option. But the output data would not be equally partitioned.</p>

<h3 id="can-you-increase-the-number-of-partitions-using-coalesce">Can you increase the number of partitions using Coalesce ?</h3>

<p>The Answer is no. Coalesce has no shuffle taking place and the algorithm is designed to move data from some partitions to existing partitions.</p>

<p>Let&rsquo;s prove the above point with an example. If we try to increase the coalesce number to 8, we would still receive the existing number of partitions present before the coalesce function, so in our case only 3 partitions would be the output.</p>

<pre><code class="language-python">df3 = df3.coalesce(8)
df3.rdd.getNumPartitions()
</code></pre>

<pre><code>3
</code></pre>

<h2 id="5-repartition-vs-coalesce-when-to-use-what">5. Repartition vs Coalesce - When to use what ?</h2>

<p>Repartition can be used under these scenarios,<br>
* when you want your output partitions to be of equally distributed chunks.
* increase the number of partitions.
* perform a shuffle of the data and create partitions.</p>

<p>Coalesce can be used under the following scenarios,<br>
* when you want to decrease the number of partitions.
* in order to avoid shuffle when decreasing partitions.</p>

<p><strong>Caution</strong>: Even though coalesce seems to be useful when decreasing partitions, it also reduces the degree of parallelism when you want to partition your data. Example, if you do an extreme coalesce to 1 partition, then all the computation would take place on a single node which is not a good practice. In this case repartition can be used.</p>

<p>Let&rsquo;s make better sense of the above caution of what happens when we perform a <code>coalesce(1)</code>. <br>
We&rsquo;ll create a dataframe with 8 partitions(default in our case) for our example and let&rsquo;s use coalesce to reduce this to a single partition</p>

<pre><code class="language-python">from pyspark.sql.types import IntegerType
df4 = spark.createDataFrame(range(10), IntegerType())

df4.coalesce(1).write.format('csv').mode('Overwrite').save('/path-to-file/sample1')
</code></pre>

<p>For the above <strong>coalesce</strong> operation, all computation takes place on a single node and not utilizing the other ones. So, in a real world problem with a huge dataset, this would create a huge overhead on a single node thereby slowing down the process and degrading the performance. The snapshot present below clearly depicts a single task being issued for the above operation,</p>

<p><img src="/image3.png" alt="image.png" /></p>

<p>Now let&rsquo;s perform the same process using repartition.</p>

<pre><code class="language-python">df4.repartition(1).write.format('csv').mode('Overwrite').save('/path-to-file/sample2')
</code></pre>

<p>For the above case, <strong>repartition</strong> first performs a full shuffle of the data. The data here resides in 8 partitions, so 8 executors are launched to perform the shuffle.</p>

<p>After completion of shuffle, the data is placed into a single node described by the second stage. So, the save operation is distributed among the different executors. Performance wise this might increase the latency a bit depending on the number of partitions created, but the overhead on a single node would be avoided. The snapshot present below indicates the same.</p>

<p><img src="/image4.png" alt="image.png" /></p>

<h2 id="6-conclusion">6. Conclusion</h2>

<p>By now, you must be clear on the concepts of partitioning in spark and how spark utilizes partitioning to perform parallel distributed processing.</p>

<p>In addition to this, the repartition and coalesce methods should be much clearer and you can use either one depending upon your use case.</p>

<p>Comments and feedback are encouraged on the comment section below. Cheers!</p>

                        </div>
                        
                        
                        <div id="comments">
                            <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "devcows" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
                        </div>
                        

                    </div>
                    

                    

                    

                    <div class="col-md-3">

                        

                        

<div class="panel panel-default sidebar-menu">

    <div class="panel-heading">
      <h3 class="panel-title">Search</h3>
    </div>

    <div class="panel-body">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" role="search">
            <div class="input-group">
                <input type="search" name="q" class="form-control" placeholder="Search">
                <input type="hidden" name="sitesearch" value="https://noufel1393.github.io">
                <span class="input-group-btn">
                    <button type="submit" class="btn btn-template-main"><i class="fa fa-search"></i></button>
                </span>
            </div>
        </form>
    </div>
</div>







<div class="panel panel-default sidebar-menu">

    <div class="panel-heading">
      <h3 class="panel-title">Categories</h3>
    </div>

    <div class="panel-body">
        <ul class="nav nav-pills nav-stacked">
            
            <li><a href="https://noufel1393.github.iocategories/lorem">lorem (1)</a>
            </li>
            
            <li><a href="https://noufel1393.github.iocategories/programming">programming (2)</a>
            </li>
            
            <li><a href="https://noufel1393.github.iocategories/pseudo">pseudo (1)</a>
            </li>
            
            <li><a href="https://noufel1393.github.iocategories/python">python (1)</a>
            </li>
            
            <li><a href="https://noufel1393.github.iocategories/r">r (1)</a>
            </li>
            
            <li><a href="https://noufel1393.github.iocategories/spark">spark (3)</a>
            </li>
            
            <li><a href="https://noufel1393.github.iocategories/starting">starting (1)</a>
            </li>
            
        </ul>
    </div>
</div>








<div class="panel sidebar-menu">
    <div class="panel-heading">
      <h3 class="panel-title">Tags</h3>
    </div>

    <div class="panel-body">
        <ul class="tag-cloud">
            
            <li><a href="https://noufel1393.github.iotags/coalesce"><i class="fa fa-tags"></i> coalesce</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/dataframe"><i class="fa fa-tags"></i> dataframe</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/go"><i class="fa fa-tags"></i> go</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/golang"><i class="fa fa-tags"></i> golang</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/hugo"><i class="fa fa-tags"></i> hugo</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/ipsum"><i class="fa fa-tags"></i> ipsum</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/multithreading"><i class="fa fa-tags"></i> multithreading</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/plot"><i class="fa fa-tags"></i> plot</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/programming"><i class="fa fa-tags"></i> programming</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/pyspark"><i class="fa fa-tags"></i> pyspark</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/python"><i class="fa fa-tags"></i> python</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/r-markdown"><i class="fa fa-tags"></i> r-markdown</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/rdd"><i class="fa fa-tags"></i> rdd</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/regression"><i class="fa fa-tags"></i> regression</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/repartition"><i class="fa fa-tags"></i> repartition</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/spark"><i class="fa fa-tags"></i> spark</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/theme"><i class="fa fa-tags"></i> theme</a>
            </li>
            
            <li><a href="https://noufel1393.github.iotags/threading"><i class="fa fa-tags"></i> threading</a>
            </li>
            
        </ul>
    </div>
</div>






                        

                    </div>
                    

                    

                </div>
                

            </div>
            
        </div>
        

        <footer id="footer">
    <div class="container">

        
        <div class="col-md-4 col-sm-6">
            <h4>About us</h4>

            Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas.

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        
        

        <div class="col-md-4 col-sm-6">

             
            <h4>Recent posts</h4>

            <div class="blog-entries">
                
                <div class="item same-height-row clearfix">
                    <div class="image same-height-always">
                        <a href="https://noufel1393.github.io/blog/2018/11/30/multithreading_in_python/">
                          
                            <img src="https://noufel1393.github.ioimg/banners/banner-3.jpg" class="img-responsive" alt="Multithreading in Python">
                          
                        </a>
                    </div>
                    <div class="name same-height-always">
                        <h5><a href="https://noufel1393.github.io/blog/2018/11/30/multithreading_in_python/">Multithreading in Python</a></h5>
                    </div>
                </div>
                
                <div class="item same-height-row clearfix">
                    <div class="image same-height-always">
                        <a href="https://noufel1393.github.io/blog/2018/11/30/pyspark_dataframe_operations/">
                          
                            <img src="https://noufel1393.github.ioimg/banners/banner-3.jpg" class="img-responsive" alt="Pyspark DataFrame Operations - Basics">
                          
                        </a>
                    </div>
                    <div class="name same-height-always">
                        <h5><a href="https://noufel1393.github.io/blog/2018/11/30/pyspark_dataframe_operations/">Pyspark DataFrame Operations - Basics</a></h5>
                    </div>
                </div>
                
                <div class="item same-height-row clearfix">
                    <div class="image same-height-always">
                        <a href="https://noufel1393.github.io/blog/2018/11/22/semi_structured_data_spark/">
                          
                            <img src="https://noufel1393.github.ioimg/banners/banner-3.jpg" class="img-responsive" alt="Semi-Structured Data in Spark (pyspark) - JSON">
                          
                        </a>
                    </div>
                    <div class="name same-height-always">
                        <h5><a href="https://noufel1393.github.io/blog/2018/11/22/semi_structured_data_spark/">Semi-Structured Data in Spark (pyspark) - JSON</a></h5>
                    </div>
                </div>
                
            </div>

            <hr class="hidden-md hidden-lg">
             

        </div>
        

        
        <div class="col-md-4 col-sm-6">

          <h4>Contact</h4>

            <strong>Universal Ltd.</strong>
        <br>13/25 New Avenue
        <br>Newtown upon River
        <br>45Y 73J
        <br>England
        <br>
        <strong>Great Britain</strong>
      </p>
      


            <a href="/contact" class="btn btn-small btn-template-main">Go to contact page</a>

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        
        

    </div>
    
</footer>







<div id="copyright">
    <div class="container">
        <div class="col-md-12">
            
            <p class="pull-left">Copyright (c) 2015 - 2016, YourCompany; all rights reserved.</p>
            
            <p class="pull-right">
              Template by <a href="http://bootstrapious.com/free-templates">Bootstrapious</a>.
              

              Ported to Hugo by <a href="https://github.com/devcows/hugo-universal-theme">DevCows</a>
            </p>
        </div>
    </div>
</div>





    </div>
    

    
<script src="//code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0/jquery.counterup.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-parallax/1.1.3/jquery-parallax.js"></script>

<script src="//maps.googleapis.com/maps/api/js?key=AIzaSyCFhtWLJcE30xOAjcbSFi-0fnoVmQZPb1Y&v=3.exp"></script>

<script src="https://noufel1393.github.iojs/hpneo.gmaps.js"></script>
<script src="https://noufel1393.github.iojs/gmaps.init.js"></script>
<script src="https://noufel1393.github.iojs/front.js"></script>


<script src="https://noufel1393.github.iojs/owl.carousel.min.js"></script>


  </body>
</html>
